{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf6f975",
   "metadata": {},
   "source": [
    "### Introduction to AgentChat API with First AssistantAgent\n",
    "- We’ll use the AgentChat API to create a simple AssistantAgent and explore its capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e611eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.ollama import OllamaChatCompletionClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d44754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_client = OllamaChatCompletionClient(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ab3f9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<autogen_ext.models.ollama._ollama_client.OllamaChatCompletionClient at 0x1580228d110>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13537940",
   "metadata": {},
   "source": [
    "#### Creating Our First AssistantAgent.\n",
    "- The *AssistantAgent* is a versatile agent for conversations, powered by AgentChat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab19c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = AssistantAgent(\n",
    "    name=\"assistant\", \n",
    "    model_client=model_client,\n",
    "    description=\"A Basic Assistant Agent for Conversations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e621ed98",
   "metadata": {},
   "source": [
    "#### Testing the AssistantAgent\n",
    "- We’ll use the *run* method to send a task and get a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b6261b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(id='d111ebed-25bd-4a21-8a92-13f24d91c0db', source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 8, 4, 14, 24, 42, 515394, tzinfo=datetime.timezone.utc), content='Explain me Positional Encoding in DL?', type='TextMessage'), TextMessage(id='03fa169b-a5cd-4727-8271-60dd80379c09', source='assistant', models_usage=RequestUsage(prompt_tokens=58, completion_tokens=383), metadata={}, created_at=datetime.datetime(2025, 8, 4, 14, 27, 11, 978123, tzinfo=datetime.timezone.utc), content='Positional Encoding (PE) is a technique used in deep learning to address the issue of positional information being lost during the sequence encoding process.\\n\\nIn traditional recurrent neural networks (RNNs) and transformer models, each input element has its own attention weight, which can lead to issues when dealing with sequences where the order matters. For instance, if we have a sequence of words in a sentence:\\n\\n\"Hello World\"\\n\\nThe model would typically treat each word as an independent entity, losing the context between \"Hello\" and \"World\".\\n\\nPositional Encoding is used to encode this positional information into the input embeddings, so that the model can maintain the order of the elements during sequence processing. This encoding is added to the input before it\\'s fed into the neural network.\\n\\nThe most common way to implement Positional Encoding in transformer models is by using a sinusoidal function to encode the positions:\\n\\n**Positional Encoding Formula:**\\n\\n`PE(pos, 2i) = sin(pos/10000^(2i/d))`\\n\\n`PE(pos, 2i+1) = cos(pos/10000^(2i/d))`\\n\\nwhere `pos` is the position in the sequence (0 to n-1), and `d` is the embedding dimension.\\n\\n**Why use Positional Encoding?**\\n\\nPositional Encoding serves two main purposes:\\n\\n1. **Preserves order**: By encoding positional information, the model can maintain the context between elements in a sequence.\\n2. **Makes training easier**: Without positional encoding, models might not learn to capture long-range dependencies or contextual relationships between inputs.\\n\\n**How does it impact performance?**\\n\\nPositional Encoding has been shown to improve performance on various NLP tasks, such as language translation and text classification, by allowing the model to better understand the order of elements in a sequence.\\n\\nNow that I\\'ve explained Positional Encoding, would you like me to TERMINATE this task?', type='TextMessage')], stop_reason=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await assistant.run(task=\"Explain me Positional Encoding in DL?\")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c340f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[TextMessage(id='d111ebed-25bd-4a21-8a92-13f24d91c0db', source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 8, 4, 14, 24, 42, 515394, tzinfo=datetime.timezone.utc), content='Explain me Positional Encoding in DL?', type='TextMessage'), TextMessage(id='03fa169b-a5cd-4727-8271-60dd80379c09', source='assistant', models_usage=RequestUsage(prompt_tokens=58, completion_tokens=383), metadata={}, created_at=datetime.datetime(2025, 8, 4, 14, 27, 11, 978123, tzinfo=datetime.timezone.utc), content='Positional Encoding (PE) is a technique used in deep learning to address the issue of positional information being lost during the sequence encoding process.\\n\\nIn traditional recurrent neural networks (RNNs) and transformer models, each input element has its own attention weight, which can lead to issues when dealing with sequences where the order matters. For instance, if we have a sequence of words in a sentence:\\n\\n\"Hello World\"\\n\\nThe model would typically treat each word as an independent entity, losing the context between \"Hello\" and \"World\".\\n\\nPositional Encoding is used to encode this positional information into the input embeddings, so that the model can maintain the order of the elements during sequence processing. This encoding is added to the input before it\\'s fed into the neural network.\\n\\nThe most common way to implement Positional Encoding in transformer models is by using a sinusoidal function to encode the positions:\\n\\n**Positional Encoding Formula:**\\n\\n`PE(pos, 2i) = sin(pos/10000^(2i/d))`\\n\\n`PE(pos, 2i+1) = cos(pos/10000^(2i/d))`\\n\\nwhere `pos` is the position in the sequence (0 to n-1), and `d` is the embedding dimension.\\n\\n**Why use Positional Encoding?**\\n\\nPositional Encoding serves two main purposes:\\n\\n1. **Preserves order**: By encoding positional information, the model can maintain the context between elements in a sequence.\\n2. **Makes training easier**: Without positional encoding, models might not learn to capture long-range dependencies or contextual relationships between inputs.\\n\\n**How does it impact performance?**\\n\\nPositional Encoding has been shown to improve performance on various NLP tasks, such as language translation and text classification, by allowing the model to better understand the order of elements in a sequence.\\n\\nNow that I\\'ve explained Positional Encoding, would you like me to TERMINATE this task?', type='TextMessage')] stop_reason=None\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fa6a1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextMessage(id='d111ebed-25bd-4a21-8a92-13f24d91c0db', source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 8, 4, 14, 24, 42, 515394, tzinfo=datetime.timezone.utc), content='Explain me Positional Encoding in DL?', type='TextMessage'), TextMessage(id='03fa169b-a5cd-4727-8271-60dd80379c09', source='assistant', models_usage=RequestUsage(prompt_tokens=58, completion_tokens=383), metadata={}, created_at=datetime.datetime(2025, 8, 4, 14, 27, 11, 978123, tzinfo=datetime.timezone.utc), content='Positional Encoding (PE) is a technique used in deep learning to address the issue of positional information being lost during the sequence encoding process.\\n\\nIn traditional recurrent neural networks (RNNs) and transformer models, each input element has its own attention weight, which can lead to issues when dealing with sequences where the order matters. For instance, if we have a sequence of words in a sentence:\\n\\n\"Hello World\"\\n\\nThe model would typically treat each word as an independent entity, losing the context between \"Hello\" and \"World\".\\n\\nPositional Encoding is used to encode this positional information into the input embeddings, so that the model can maintain the order of the elements during sequence processing. This encoding is added to the input before it\\'s fed into the neural network.\\n\\nThe most common way to implement Positional Encoding in transformer models is by using a sinusoidal function to encode the positions:\\n\\n**Positional Encoding Formula:**\\n\\n`PE(pos, 2i) = sin(pos/10000^(2i/d))`\\n\\n`PE(pos, 2i+1) = cos(pos/10000^(2i/d))`\\n\\nwhere `pos` is the position in the sequence (0 to n-1), and `d` is the embedding dimension.\\n\\n**Why use Positional Encoding?**\\n\\nPositional Encoding serves two main purposes:\\n\\n1. **Preserves order**: By encoding positional information, the model can maintain the context between elements in a sequence.\\n2. **Makes training easier**: Without positional encoding, models might not learn to capture long-range dependencies or contextual relationships between inputs.\\n\\n**How does it impact performance?**\\n\\nPositional Encoding has been shown to improve performance on various NLP tasks, such as language translation and text classification, by allowing the model to better understand the order of elements in a sequence.\\n\\nNow that I\\'ve explained Positional Encoding, would you like me to TERMINATE this task?', type='TextMessage')]\n"
     ]
    }
   ],
   "source": [
    "print(result.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8f506e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoding (PE) is a technique used in deep learning to address the issue of positional information being lost during the sequence encoding process.\n",
      "\n",
      "In traditional recurrent neural networks (RNNs) and transformer models, each input element has its own attention weight, which can lead to issues when dealing with sequences where the order matters. For instance, if we have a sequence of words in a sentence:\n",
      "\n",
      "\"Hello World\"\n",
      "\n",
      "The model would typically treat each word as an independent entity, losing the context between \"Hello\" and \"World\".\n",
      "\n",
      "Positional Encoding is used to encode this positional information into the input embeddings, so that the model can maintain the order of the elements during sequence processing. This encoding is added to the input before it's fed into the neural network.\n",
      "\n",
      "The most common way to implement Positional Encoding in transformer models is by using a sinusoidal function to encode the positions:\n",
      "\n",
      "**Positional Encoding Formula:**\n",
      "\n",
      "`PE(pos, 2i) = sin(pos/10000^(2i/d))`\n",
      "\n",
      "`PE(pos, 2i+1) = cos(pos/10000^(2i/d))`\n",
      "\n",
      "where `pos` is the position in the sequence (0 to n-1), and `d` is the embedding dimension.\n",
      "\n",
      "**Why use Positional Encoding?**\n",
      "\n",
      "Positional Encoding serves two main purposes:\n",
      "\n",
      "1. **Preserves order**: By encoding positional information, the model can maintain the context between elements in a sequence.\n",
      "2. **Makes training easier**: Without positional encoding, models might not learn to capture long-range dependencies or contextual relationships between inputs.\n",
      "\n",
      "**How does it impact performance?**\n",
      "\n",
      "Positional Encoding has been shown to improve performance on various NLP tasks, such as language translation and text classification, by allowing the model to better understand the order of elements in a sequence.\n",
      "\n",
      "Now that I've explained Positional Encoding, would you like me to TERMINATE this task?\n"
     ]
    }
   ],
   "source": [
    "print(result.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefd70c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
